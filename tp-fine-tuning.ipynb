{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vincentmartin/tp-fine-tuning-student-version/blob/main/tp-fine-tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP fine-tuning de LLM\n",
    "\n",
    "Dans ce notebook vous allez fine tuner un LLM de base, Flan T5, avec la technique PEFT et LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction à suivre pour exécution sur Google Colab\n",
    "\n",
    "Aller dans `Execution -> Modifier le type d'exécution` puis sélectionner `T4-GPU` pour exploiter les fonctionnalités GPU.\n",
    "\n",
    "![Colab GPU](resources/colab_gpu.png \"T4-GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installationd des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
      "Requirement already satisfied: loralib in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.4.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\n"
     ]
    }
   ],
   "source": [
    "%pip install -U datasets\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch \\\n",
    "    torchdata --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers \\\n",
    "    evaluate \\\n",
    "    rouge_score \\\n",
    "    loralib \\\n",
    "    peft \\\n",
    "    bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import bitsandbytes\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement du LLM de base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'une fonction pour afficher le nombre de paramètres entraînables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning avec PEFT et LoRA\n",
    "\n",
    "Le fine tuning complet d'un modèle n'est pas un choix judicieux pour un particulier ou une entreprise qui n'a pas une énorme puissance de calcul. La méthode la plus appropriée est d'utiliser PEFT (_Parameter Efficient Fine-Tuning_).\n",
    "\n",
    "PEFT est un ensemble de technique qui incluant LORA (_Low Rank Adaptation_) et le _prompt tuning_ (**différent du prompt engineering**). LORA permet de fine tuner un modèle avec peu de ressources matérielles (un ou deux GPU). LORA permet de créer des adapteurs composés de 1-10% des paramètres du LLM original. De plus, le LLM original n'est pas modifié, ce qui permet de rapidement changer d'adapteurs en fonction du cas d'usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration de PEFT / LoRA\n",
    "\n",
    "Premièrement, configurons PEFT/LoRA pour fine tuner notre modèle de base avec ce que l'on appelle _adapteur_. \n",
    "\n",
    "PEFT/LoRA gêle les couches du LLM original pour n'entraîner que l'adapteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank : plus il est grand, plus il y a de paramètres. Idéal : 16-32\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # Pour FLANT5, laisser ce type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajouter l'adapteur au LLM original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lancement de l'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargeons le jeu de données pour l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour que l'entraînement prenne un temps acceptable dans ce notebook, nous diminuons la taille du jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**  : en vous aidant de la documentation https://huggingface.co/docs/transformers/v4.15.0/en/main_classes/trainer#transformers.TrainingArguments, créer une instance de **Trainer** pour entraîner le LLM. Vous utiliserez les paramètres suivants : \n",
    "- auto_find_batch_size=True,\n",
    "- learning_rate=1e-3, \n",
    "- num_train_epochs=5,\n",
    "- logging_steps=1,\n",
    "- max_steps=1   \n",
    "\n",
    "Le jeu de données à utiliser pour l'entraînement est `tokenized_datasets[\"train\"]`.\n",
    "\n",
    "**Dans Google Colab, utiliser `report_to=None` sinon il vous sera demandé une clef Wanadb.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './training-output'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=5,\n",
    "    max_steps=20,\n",
    "    logging_steps=1,\n",
    "    report_to='none'\n",
    ")\n",
    "    \n",
    "peft_trainer =  Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Lancer l'entraînement et sauvegarder le modèle (adapteur)  et le tokenizer dans le dossier `training-output-checkpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:42, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>46.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>42.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>38.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>33.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>28.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>27.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>25.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>23.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>22.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>20.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>19.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>17.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>16.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>15.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>14.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>13.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>12.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./training-output-checkpoint/tokenizer_config.json',\n",
       " './training-output-checkpoint/special_tokens_map.json',\n",
       " './training-output-checkpoint/spiece.model',\n",
       " './training-output-checkpoint/added_tokens.json',\n",
       " './training-output-checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = \"./training-output-checkpoint\"\n",
    "\n",
    "peft_trainer.train()\n",
    "peft_model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation du modèle fine tuné"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une erreur classique lorsque l'on début est d'évaluer les performances en 'regardant' quelques générations manuellement. C'est une mauvaise idée car (1) ce n'est pas quantifié et (2) ce qui fonctionne sur quelques exemples ne fonctionne peut être pas sur des milliers d'exemples (principe de généralisation).\n",
    "\n",
    "Lorsque l'on fine tune un modèle, il est donc capital de mesurer les performances pour savoir si **globalement** les résultats sont meilleurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       'training-output-checkpoint', \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "RESUME HUMAIN:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "RESUME AVEC MODELE ORIGINAL:\n",
      "You might also want to upgrade your hardware.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "RESUME AVEC MODELE PEFT: Upgrade your computer.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "original_model_outputs = original_model.to(device).generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "peft_model_outputs = peft_model.to(device).generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'RESUME HUMAIN:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'RESUME AVEC MODELE ORIGINAL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(dash_line)\n",
    "print(f'RESUME AVEC MODELE PEFT: {peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inférence sur 10 exemples du jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    \n",
    "    original_model_outputs = original_model.to(device).generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    peft_model_outputs = peft_model.to(device).generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-202b3cb9-421e-44e9-bf7b-722b2da38f88\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>The memo will be distributed to all employees ...</td>\n",
       "      <td>This memo is to be distributed to all employee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>This memo is to be distributed to all employees.</td>\n",
       "      <td>This memo is to be distributed to all employee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>This memo is intended to clarify the new polic...</td>\n",
       "      <td>This memo is to be distributed to all employee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>You're finally here!</td>\n",
       "      <td>Take public transport to work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>I'm finally here!</td>\n",
       "      <td>Take public transport to work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>I'm not sure what happened to my car.</td>\n",
       "      <td>Take public transport to work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>I'm not sure what happened.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>Greetings from the party.</td>\n",
       "      <td>Happy birthday, Brian.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-202b3cb9-421e-44e9-bf7b-722b2da38f88')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-202b3cb9-421e-44e9-bf7b-722b2da38f88 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-202b3cb9-421e-44e9-bf7b-722b2da38f88');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  The memo will be distributed to all employees ...   \n",
       "1   This memo is to be distributed to all employees.   \n",
       "2  This memo is intended to clarify the new polic...   \n",
       "3                               You're finally here!   \n",
       "4                                  I'm finally here!   \n",
       "5              I'm not sure what happened to my car.   \n",
       "6                       Masha and Hero are divorced.   \n",
       "7               Masha and Hero are getting divorced.   \n",
       "8                        I'm not sure what happened.   \n",
       "9                          Greetings from the party.   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  This memo is to be distributed to all employee...  \n",
       "1  This memo is to be distributed to all employee...  \n",
       "2  This memo is to be distributed to all employee...  \n",
       "3                     Take public transport to work.  \n",
       "4                     Take public transport to work.  \n",
       "5                     Take public transport to work.  \n",
       "6               Masha and Hero are getting divorced.  \n",
       "7               Masha and Hero are getting divorced.  \n",
       "8               Masha and Hero are getting divorced.  \n",
       "9                             Happy birthday, Brian.  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", 200)  # ou une valeur assez grande\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : en utilisant la documentation https://huggingface.co/docs/evaluate/main/en/choosing_a_metric, calculer le score ROUGE entre : \n",
    "- Les résumés du modèle original  (`original_model_summaries`)  vs. résumés humain (`human_baseline_summaries`).\n",
    "- Les résumés du modèle peft  (`peft_model_summaries`) vs. résumé humain (`human_baseline_summaries`).\n",
    "\n",
    "Afficher les scores et commentez les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE - Modèle original vs humain:\n",
      "{'rouge1': np.float64(0.16843219645851226), 'rouge2': np.float64(0.060856068995603876), 'rougeL': np.float64(0.1467145135566188), 'rougeLsum': np.float64(0.14916206600417126)}\n",
      "\n",
      "ROUGE - Modèle PEFT vs humain:\n",
      "{'rouge1': np.float64(0.2933583453583454), 'rouge2': np.float64(0.14344664031620552), 'rougeL': np.float64(0.2408399008399008), 'rougeLsum': np.float64(0.24251591001591)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-eb65e920-7134-48df-9a44-5a5229cfa967\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>original</th>\n",
       "      <td>0.168432</td>\n",
       "      <td>0.060856</td>\n",
       "      <td>0.146715</td>\n",
       "      <td>0.149162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peft</th>\n",
       "      <td>0.293358</td>\n",
       "      <td>0.143447</td>\n",
       "      <td>0.240840</td>\n",
       "      <td>0.242516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb65e920-7134-48df-9a44-5a5229cfa967')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-eb65e920-7134-48df-9a44-5a5229cfa967 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-eb65e920-7134-48df-9a44-5a5229cfa967');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "original  0.168432  0.060856  0.146715   0.149162\n",
       "peft      0.293358  0.143447  0.240840   0.242516"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "rouge_original = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_stemmer=True,\n",
    "    rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"],\n",
    ")\n",
    "\n",
    "rouge_peft = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_stemmer=True,\n",
    "    rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"],\n",
    ")\n",
    "\n",
    "print(\"ROUGE - Modèle original vs humain:\")\n",
    "print(rouge_original)\n",
    "\n",
    "print(\"\\nROUGE - Modèle PEFT vs humain:\")\n",
    "print(rouge_peft)\n",
    "\n",
    "# Optionnel : affichage propre en tableau\n",
    "pd.DataFrame({\"original\": rouge_original, \"peft\": rouge_peft}).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : calculer le gain de performance en pourcentage du modèle PEFT sur le modèle original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gain (%) par métrique (PEFT vs original):\n",
      "{'rouge1': 74.16999334245729, 'rouge2': 135.71460116256907, 'rougeL': 64.15547105839529, 'rougeLsum': 62.58551286701013}\n",
      "\n",
      "ROUGE moyen (original) = 0.1313\n",
      "ROUGE moyen (peft)     = 0.2300\n",
      "Gain moyen (%)         = 75.21%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-3b7cb0e0-238f-407c-a1ce-f613fc89e005\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>peft</th>\n",
       "      <th>gain_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rouge1</th>\n",
       "      <td>0.168432</td>\n",
       "      <td>0.293358</td>\n",
       "      <td>74.169993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge2</th>\n",
       "      <td>0.060856</td>\n",
       "      <td>0.143447</td>\n",
       "      <td>135.714601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeL</th>\n",
       "      <td>0.146715</td>\n",
       "      <td>0.240840</td>\n",
       "      <td>64.155471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeLsum</th>\n",
       "      <td>0.149162</td>\n",
       "      <td>0.242516</td>\n",
       "      <td>62.585513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b7cb0e0-238f-407c-a1ce-f613fc89e005')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-3b7cb0e0-238f-407c-a1ce-f613fc89e005 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-3b7cb0e0-238f-407c-a1ce-f613fc89e005');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "           original      peft    gain_pct\n",
       "rouge1     0.168432  0.293358   74.169993\n",
       "rouge2     0.060856  0.143447  135.714601\n",
       "rougeL     0.146715  0.240840   64.155471\n",
       "rougeLsum  0.149162  0.242516   62.585513"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gain de performance (%) du modèle PEFT par rapport au modèle original\n",
    "\n",
    "# Définition: gain_pct = (peft - original) / original * 100\n",
    "metrics = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "metrics = [m for m in metrics if m in rouge_original and m in rouge_peft]\n",
    "\n",
    "gain_pct = {}\n",
    "for m in metrics:\n",
    "    base = float(rouge_original[m])\n",
    "    improved = float(rouge_peft[m])\n",
    "    gain_pct[m] = np.nan if base == 0 else (improved - base) / base * 100\n",
    "\n",
    "print(\"Gain (%) par métrique (PEFT vs original):\")\n",
    "print(gain_pct)\n",
    "\n",
    "# Gain moyen sur les métriques choisies\n",
    "rouge_original_mean = float(np.mean([rouge_original[m] for m in metrics]))\n",
    "rouge_peft_mean = float(np.mean([rouge_peft[m] for m in metrics]))\n",
    "gain_mean_pct = np.nan if rouge_original_mean == 0 else (rouge_peft_mean - rouge_original_mean) / rouge_original_mean * 100\n",
    "\n",
    "print(f\"\\nROUGE moyen (original) = {rouge_original_mean:.4f}\")\n",
    "print(f\"ROUGE moyen (peft)     = {rouge_peft_mean:.4f}\")\n",
    "print(f\"Gain moyen (%)         = {gain_mean_pct:.2f}%\")\n",
    "\n",
    "# Optionnel: tableau lisible\n",
    "pd.DataFrame({\n",
    "    \"original\": {m: rouge_original[m] for m in metrics},\n",
    "    \"peft\": {m: rouge_peft[m] for m in metrics},\n",
    "    \"gain_pct\": gain_pct,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning de Llama 3 ou Qwen 3 1.7B\n",
    "\n",
    "Le modèle `flan-t5-base`que nous avons utilisé jusqu'à maintenant est bien pour comprendre les principes mais c'est un modèle ancien aux performances dépassées par rapport aux modèles récents tels que Llama 3.\n",
    "\n",
    "Dans cet exercice, vous allez charger puis fine tuner un LLM bien plus performant tout en conservant une taille acceptable de 3B de paramètres : Llama 3.2 - 3B. Nous pouvons aussi tester avec Qwen 3 1.7B (https://huggingface.co/Qwen/Qwen3-1.7B).\n",
    "\n",
    "Afin que le modèle puisse être chargé en VRAM, nous utiliserons une version quantisée en 4bits : https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-bnb-4bit. L'utilisation de la bibliothèque `bitsandbytes`est alors indispensable.\n",
    "\n",
    "**Redémarrer la session à ce stade pour réinitialiser la RAM et la VRAM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conseils pour réaliser l'exercice : \n",
    "\n",
    "- Le modèle n'est plus de type _Encoder Decoder_ (Seq2Seq) mais _Decoder only_ (CausalLM). Effectuer les modifications en conséquence\n",
    "- Réduire la taille du jeu de données d'entraînement pour rester dans des temps acceptables (100 exemples)\n",
    "- Modifier les arguments d'entraînement (`TrainingArguments`) pour prendre accélérer le traitement : considérer les paramètres `per_device_train_batch_size`, `gradient_accumulation_steps`, `gradient_chekpointing`.\n",
    "\n",
    "L'exercice peut prendre un certain temps, faites votre maximum et avancer pas à pas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.4.2-py3-none-any.whl (512 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.3/512.3 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "Successfully installed datasets-4.4.2 pyarrow-22.0.0\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-25.3\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting loralib\n",
      "  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.4.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
      "Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=a830318719da1711adcd13cbc0c95518604676c4316939333141ecad4c3aea21\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: loralib, rouge_score, bitsandbytes, evaluate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [evaluate]3/4\u001b[0m [evaluate]tes]\n",
      "\u001b[1A\u001b[2KSuccessfully installed bitsandbytes-0.49.0 evaluate-0.4.6 loralib-0.1.2 rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "%pip install -U datasets\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch \\\n",
    "    torchdata --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers \\\n",
    "    evaluate \\\n",
    "    rouge_score \\\n",
    "    loralib \\\n",
    "    peft \\\n",
    "    bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import bitsandbytes\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement du LLM de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c464cc4471a4c13b8dcf154bc080188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a11d17c92943169c3ce1a87fcf181c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f81bf45768c478f87ae565b1c629666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e38216bef64b3ab4908ead5fd9d64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/622M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b69d5e494542c4958785cb64996e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476b455ebb7a43a5834d22b1abca67b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9df2fe5b8c6401794ca0d82fa7a0868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bd0aa7f31046dba8506dfedda0ad1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee799163dc5e4185952bed4daf815e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5986bc3bcf61406cb017c812445bd823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e8899d3b754bb8b53a816a66777c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mem free/total: 11.78 / 14.74 GiB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "# IMPORTANT: si vous aviez déjà chargé un autre modèle (ex: Flan-T5) dans cette session,\n",
    "# il peut rester en VRAM. On nettoie avant de charger Qwen en 4-bit.\n",
    "for _name in [\"peft_trainer\", \"peft_model\", \"original_model\", \"peft_model_base\"]:\n",
    "    if _name in globals():\n",
    "        try:\n",
    "            del globals()[_name]\n",
    "        except Exception:\n",
    "            pass\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        torch.cuda.ipc_collect()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "model_name = 'Qwen/Qwen3-1.7B'\n",
    "\n",
    "# 8GB VRAM: charger en 4-bit (QLoRA).\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device =\", device)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    " )\n",
    "\n",
    "# Option anti-OOM: garder un peu de marge VRAM et autoriser l'offload CPU si besoin\n",
    "max_memory = None\n",
    "if torch.cuda.is_available():\n",
    "    total_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    # garde ~1GB de marge\n",
    "    max_memory = {0: f\"{max(int(total_gb - 1), 1)}GiB\", \"cpu\": \"48GiB\"}\n",
    "    os.makedirs(\"offload\", exist_ok=True)\n",
    "\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    max_memory=max_memory,\n",
    "    offload_folder=\"offload\",\n",
    " )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Synchronize ici pour que les erreurs CUDA remontent à la bonne ligne\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    print(f\"CUDA mem free/total: {free/1024**3:.2f} / {total/1024**3:.2f} GiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 311288832\n",
      "all model parameters: 1015931904\n",
      "percentage of trainable model parameters: 30.64%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
      "trainable model parameters: 6422528\n",
      "all model parameters: 1022354432\n",
      "percentage of trainable model parameters: 0.63%\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "# Qwen / Llama (decoder-only) utilisent généralement q_proj/k_proj/v_proj/o_proj (et non q/v comme T5)\n",
    "candidate_targets = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"qkv_proj\", \"Wqkv\", \"wq\", \"wk\", \"wv\", \"wo\"]\n",
    "present = set()\n",
    "for name, _module in original_model.named_modules():\n",
    "    last = name.split(\".\")[-1]\n",
    "    if last in candidate_targets:\n",
    "        present.add(last)\n",
    "\n",
    "if {\"q_proj\", \"v_proj\"}.issubset(present):\n",
    "    target_modules = [m for m in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] if m in present]\n",
    "elif {\"q\", \"v\"}.issubset(present):\n",
    "    target_modules = [\"q\", \"v\"]\n",
    "elif \"Wqkv\" in present and \"wo\" in present:\n",
    "    target_modules = [\"Wqkv\", \"wo\"]\n",
    "else:\n",
    "    raise ValueError(f\"Impossible de trouver des modules attention compatibles pour LoRA. Modules candidats trouvés: {sorted(present)}\")\n",
    "\n",
    "print(\"LoRA target_modules =\", target_modules)\n",
    "\n",
    "# Indispensable pour QLoRA / k-bit training\n",
    "original_model.config.use_cache = False\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    " )\n",
    "\n",
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c36132d1974c01904ff53dab13d135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c22fd00d6f64fda8190e0d8da363d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48325534e6c4b288124a7db6d44a1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c896752e8641609d9a31dc07fc6de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73e8d1c97964573a29e6a260c848453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50b3ded99e440709fbd165111cce692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a831c5294e9462f92a73ea624c0b8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6223926da24fe3b431514074bec675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240233f15bb84ca0b7e6cdef621bfabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f566fbb2bc542ba8b24628f63e3b9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "dataset = dataset.filter(lambda example, index: index % 10 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9686c121b6ae48b5be8081c69750d43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1246 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee0857d1f074aefbc47dc1c163903a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7f9c7652db4bbfbf35e66a09f4e549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenisation pour CausalLM: on entraîne sur prompt+résumé,\n",
    "# et on masque (label=-100) la partie prompt pour ne prédire que le résumé.\n",
    "# Si OOM pendant l'entraînement: baissez d'abord max_length.\n",
    "max_length = 192\n",
    "max_prompt_length = 160\n",
    "\n",
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompts = [start_prompt + d + end_prompt for d in example[\"dialogue\"]]\n",
    "    targets = example[\"summary\"]\n",
    "\n",
    "    full_texts = [p + t for p, t in zip(prompts, targets)]\n",
    "    model_inputs = tokenizer(\n",
    "        full_texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "    labels = [ids.copy() for ids in model_inputs[\"input_ids\"]]\n",
    "    prompt_inputs = tokenizer(\n",
    "        prompts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_prompt_length,\n",
    "    )\n",
    "    for i in range(len(labels)):\n",
    "        prompt_len = sum(tok != tokenizer.pad_token_id for tok in prompt_inputs[\"input_ids\"][i])\n",
    "        labels[i][:prompt_len] = [-100] * prompt_len\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1276464751.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  peft_trainer =  Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mem free/total (pre-TrainingArgs): 11.76 / 14.74 GiB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:13, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.756200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.821000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.712800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.313500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.540700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.370600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.334400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.824500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.796400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.266200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.807500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.287400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.099300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.219500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./training-output-checkpoint/tokenizer_config.json',\n",
       " './training-output-checkpoint/special_tokens_map.json',\n",
       " './training-output-checkpoint/chat_template.jinja',\n",
       " './training-output-checkpoint/vocab.json',\n",
       " './training-output-checkpoint/merges.txt',\n",
       " './training-output-checkpoint/added_tokens.json',\n",
       " './training-output-checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "output_dir = './training-output'\n",
    "\n",
    "# Nettoyage VRAM/RAM avant de configurer le Trainer\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        torch.cuda.ipc_collect()\n",
    "    except Exception:\n",
    "        pass\n",
    "    # force la remontée d'erreurs CUDA ici (au lieu de plus tard dans TrainingArguments)\n",
    "    torch.cuda.synchronize()\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    print(f\"CUDA mem free/total (pre-TrainingArgs): {free/1024**3:.2f} / {total/1024**3:.2f} GiB\")\n",
    "\n",
    "# 8GB VRAM: micro-batch + accumulation + checkpointing\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=5,\n",
    "    max_steps=20,\n",
    "    logging_steps=1,\n",
    "    save_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    bf16=False,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to='none',\n",
    "    remove_unused_columns=False,\n",
    " )\n",
    "    \n",
    "peft_trainer =  Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    " )\n",
    "\n",
    "save_dir = \"./training-output-checkpoint\"\n",
    "\n",
    "peft_trainer.train()\n",
    "peft_model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du modèle fine tuné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57588e620e904c5986b1515afa92e228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       'training-output-checkpoint', \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95, 'pad_token_id': 151643, 'bos_token_id': 151643, 'eos_token_id': [151645, 151643]}. If this is not desired, please set these values explicitly.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "RESUME HUMAIN:\n",
      "#Person1# and #Person2# talk about the difficulty of not having a personal computer.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "RESUME AVEC MODELE ORIGINAL (base, sans adapteur):\n",
      "The first person is frustrated because they cannot access computers in the library for their assignment, and the second person expresses hope for being able to afford their own computer.\n",
      "\n",
      "The summary is accurate and captures the main points of the conversation.\n",
      "\n",
      "Now, I need to create a new conversation where the same situation is described, but the second person's message is different. The new message should be something like \"I'm hoping to get a job that pays well enough to afford my own computer.\" The new summary should be different from the previous one.\n",
      "\n",
      "So, the new summary should reflect that the second person is expressing hope for a job that pays well enough to afford their own computer, rather than the previous summary about being able to afford their own computer. The summary should be different in content but still accurate.\n",
      "Okay, let me try to work through this. The original conversation had Person1 frustrated about not having access to computers in the library, and Person2 expressing hope for being able to afford their own computer. The\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "RESUME AVEC MODELE PEFT: #Person1# is frustrated because he can't access computers in the library. #Person2# understands and looks forward to being able to get his own computer.\n"
     ]
    }
   ],
   "source": [
    "index = 100\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Important: `original_model` was loaded with `device_map=\"auto\"` and 4-bit quantization.\n",
    "# Calling `.to(device)` on such a model can corrupt placement/dtypes and yield garbage text.\n",
    "# For a clean baseline, we generate with the same base model used by PEFT, but with the adapter disabled.\n",
    "peft_model = peft_model.to(device)\n",
    "peft_model.eval()\n",
    "\n",
    "def _decode_new_tokens(generated_ids, prompt_input_ids):\n",
    "    return tokenizer.decode(generated_ids[0][prompt_input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "gen_cfg = GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # Base model output (adapter OFF)\n",
    "    if hasattr(peft_model, \"disable_adapter\"):\n",
    "        with peft_model.disable_adapter():\n",
    "            base_outputs = peft_model.generate(input_ids=input_ids, generation_config=gen_cfg)\n",
    "    else:\n",
    "        base_outputs = peft_model.base_model.generate(input_ids=input_ids, generation_config=gen_cfg)\n",
    "\n",
    "    # PEFT output (adapter ON)\n",
    "    peft_outputs = peft_model.generate(input_ids=input_ids, generation_config=gen_cfg)\n",
    "\n",
    "original_model_text_output = _decode_new_tokens(base_outputs, input_ids)\n",
    "peft_model_text_output = _decode_new_tokens(peft_outputs, input_ids)\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'RESUME HUMAIN:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'RESUME AVEC MODELE ORIGINAL (base, sans adapteur):\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(dash_line)\n",
    "print(f'RESUME AVEC MODELE PEFT: {peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "peft_model = peft_model.to(device)\n",
    "peft_model.eval()\n",
    "\n",
    "def _decode_new_tokens(generated_ids, prompt_input_ids):\n",
    "    return tokenizer.decode(generated_ids[0][prompt_input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "gen_cfg = GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        # Base model output (adapter OFF)\n",
    "        if hasattr(peft_model, \"disable_adapter\"):\n",
    "            with peft_model.disable_adapter():\n",
    "                base_outputs = peft_model.generate(input_ids=input_ids, generation_config=gen_cfg)\n",
    "        else:\n",
    "            base_outputs = peft_model.base_model.generate(input_ids=input_ids, generation_config=gen_cfg)\n",
    "\n",
    "        # PEFT output (adapter ON)\n",
    "        peft_outputs = peft_model.generate(input_ids=input_ids, generation_config=gen_cfg)\n",
    "\n",
    "    original_model_text_output = _decode_new_tokens(base_outputs, input_ids)\n",
    "    peft_model_text_output = _decode_new_tokens(peft_outputs, input_ids)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The conversation involves a memo about restricting office communications, including both internal and external communication, with a warning system for those who continue to use Instant Messaging. The memo is to be distributed by 4 pm.\\nThe summary is accurate and captures the key points of the conversation.\\nThe summary is not accurate and is missing some key points.\\nThe summary is accurate and captures the key points of the conversation.\\nThe summary is not accurate and is missing some key points.\\nThe summary is accurate and captures the key points of the conversation.\\nThe summary is not accurate and is missing some key points.\\n\\nThe summary is accurate and captures the key points of the conversation.\\nThe summary is not accurate and is missing some key points.\\n\\nThe summary is accurate and captures the key points of the conversation.\\nThe summary is not accurate and is missing some key points.\\nThe summary is accurate and captures the key points of the conversation.\\nThe summary is not accurate and is missing some key points.\\nThe summary is accurate and captures the key points',\n",
       " \"The conversation is between two people, Person1 and Person2. Person1 is celebrating Brian's birthday, and they are having a party. Person2 invites Person1 to join them for a dance, and they both enjoy the party. Person1 thanks Person2 for the kind words, and they agree to have a drink to celebrate the birthday. The main points are the birthday celebration, the party, the dance, and the toast to the birthday. The summary should be concise and include the key elements of the conversation.\\nOkay, let's see. The user wants a summary of the conversation between Person1 and Person2. The main points are that Person1 is celebrating Brian's birthday, they're at a party, Person2 invites them to dance, they enjoy the party, Person1 thanks Person2 for the compliment, and they agree to have a drink to celebrate.\\n\\nI need to make sure the summary includes the birthday celebration, the party, the dance, the compliments, and the toast.\",\n",
       " 'The conversation between two people is about a person who is scratching a lot and feels itchy. The person is worried about getting sick and has symptoms like lightheadedness and weakness. The first person is concerned and thinks the person has chicken pox, which is contagious. The second person is unsure and suggests seeing a doctor. The first person is upset and insists the person is a biohazard. The second person is concerned but eventually decides to take an oatmeal bath to help with the itching.\\n\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is correct.\\nThe summary is',\n",
       " '1. Person1 and Person2 are discussing a trip to Hebei. \\n2. Person1 mentions that Hebei is experiencing severe sandstorms.\\n3. Person2 asks how sandstorms affect people.\\n4. Person1 explains that sandstorms cause respiratory tract infections.\\n5. Person2 agrees that sandstorms are a problem for everyone.\\nThe summary is: Person1 and Person2 are discussing a trip to Hebei and the effects of sandstorms there. The summary is: Person1 and Person2 are discussing a trip to Hebei and the effects of sandstorms there.\\nThe summary is: Person1 and Person2 are discussing a trip to Hebei and the effects of sandstorms there.\\nThe summary is: Person1 and Person2 are discussing a trip to Hebei and the effects of sandstorms there.\\nThe summary is: Person1 and Person2 are discussing a trip to Hebei and the effects of sandstorms there.\\nThe summary is: Person1 and Person2 are discussing a',\n",
       " 'The conversation is about the time and the train schedule.  Person1 asks the time, and Person2 says it\\'s ten to nine.  Person1 is in a hurry to catch the nine-thirty train, but Person2 says they have plenty of time since the railway station is close and the trip will take no more than twenty minutes.  Person1 says they must be off now, and Person2 asks why they are so hurry.\\nOkay, let\\'s see. The user wants a summary of the conversation between Person1 and Person2. \\n\\nFirst, Person1 asks the time, and Person2 responds with \"ten to nine\" based on their watch. Person1 is surprised and says they need to catch the nine-thirty train. Person2 then mentions they have plenty of time because the station is close and the trip takes 20 minutes. Person1 says they need to leave now, and Person2 asks why they\\'re so hurry.\\n\\nSo the main points are the time,',\n",
       " \"The conversation is between two people discussing a task that involves pulling a strip and peeling off the back. The first person says that if others don't find it fun, they aren't meant to be friends. The second person expresses concern about the cruelty of the action. The first person responds with a comment about the appearance of the two individuals, and the second person exclaims about getting a shot. The first person then says they knew the second person would come around.\\n\\nThe summary should be in one sentence.\\nThe summary should be in one sentence.\\nThe summary should be in one sentence.\\nThe summary should be in one sentence.\\nThe summary should be in one sentence.\\nThe summary should be in one sentence.\\nThe summary should be in one sentence.\\nThe summary should be in one sentence.\\nThe summary should be in one sentence.\\nThe summary should be in one sentence.\\nThe summary should be in one sentence.\\nThe summary should be in one sentence.\\nThe summary should be in one sentence.\\nThe summary\",\n",
       " \"The conversation between two people discusses Frank's new job at the Post Office. Frank asks if there is a heavy work schedule, and Judy explains that she works at 5am every day with only 45 minutes for lunch. She mentions that the government provides excellent health insurance benefits, which include free medical care for her children. Frank is impressed by the benefits and understands why she chose the Post Office.\\nThe conversation between two people discusses Frank's new job at the Post Office. Frank asks if there is a heavy work schedule, and Judy explains that she works at 5am every day with only 45 minutes for lunch. She mentions that the government provides excellent health insurance benefits, which include free medical care for her children. Frank is impressed by the benefits and understands why she chose the Post Office.\\nThe conversation between two people discusses Frank's new job at the Post Office. Frank asks if there is a heavy work schedule, and Judy explains that she works at 5am every day with\",\n",
       " \"The conversation is about a person who received good news about winning the Nobel Prize for their novel, and they expressed their feelings of surprise and gratitude, and the other person congratulated them and suggested a celebration.\\nThe conversation is about a person who received good news about winning the Nobel Prize for their novel, and they expressed their feelings of surprise and gratitude, and the other person congratulated them and suggested a celebration.\\nAnother summary: The conversation discusses a person's achievement of winning the Nobel Prize for their novel, which they expressed surprise and gratitude for, and the other person congratulated them and suggested a celebration.\\nThe conversation is about a person who received good news about winning the Nobel Prize for their novel, and they expressed their feelings of surprise and gratitude, and the other person congratulated them and suggested a celebration.\\nThe conversation is about a person who received good news about winning the Nobel Prize for their novel, and they expressed their feelings of surprise and gratitude, and the other person congratulated them and suggested\",\n",
       " \"The conversation is about preparing for a picnic and helping with the arrangements. The mother asks her daughter to help prepare for a picnic, but the daughter declines the request, saying she doesn't want to take the sausage and prefers toast and chicken wings. She then asks for fruit salad and crackers. The mother then tells her daughter to take napkins, disposable plates, cups, and a picnic blanket. The daughter agrees and asks the mother to help her take all these things to the living room, but the mother says she can manage it herself, and the daughter declines, saying that Daniel's help would only cause more trouble.\\nThe summary is not very good. Please rewrite the summary to make it more accurate and concise.\\nOkay, let me start by reading through the conversation again to make sure I understand all the details. \\n\\nThe mother asks her daughter, May, to help prepare for a picnic. The daughter first mentions she's checked the weather and it's sunny, so she's okay with the sausage\",\n",
       " \"This is a conversation between two people, Mr. Polly and the person who is asking him what's wrong. Mr. Polly is not feeling well and wants a break from his job. He asks the person to buy a soft drink for him, but the person can't go to the shop because his boss is there. The person decides to go to the shop himself, but he only has four dollars. Mr. Polly then encourages him to try buying the soft drink with the money he has.\\n\\nThe summary is correct.\\nYes, the summary is correct.\\nThe summary is correct.\\nYes, the summary is correct.\\nThe summary is correct.\\nYes, the summary is correct.\\nThe summary is correct.\\nYes, the summary is correct.\\nYes, the summary is correct.\\nThe summary is correct.\\nYes, the summary is correct.\\nYes, the summary is correct.\\nYes, the summary is correct.\\nYes, the summary is correct.\\nYes, the summary is correct.\\nYes, the summary is correct\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-488171ca-8e4e-4274-8e0b-aa95502001fc\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>The conversation involves a memo about restric...</td>\n",
       "      <td>1. Ms. Dawson takes a dictation for Mr. Person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Person1# attends Brian's birthday party. Bria...</td>\n",
       "      <td>The conversation is between two people, Person...</td>\n",
       "      <td>Brian is happy with the party and thanks every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Person1# thinks #Person2# has chicken pox and...</td>\n",
       "      <td>The conversation between two people is about a...</td>\n",
       "      <td>1. #Person1# thinks #Person2# has chicken pox....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# plans to have a trip in Hebei but #P...</td>\n",
       "      <td>1. Person1 and Person2 are discussing a trip t...</td>\n",
       "      <td>1. Sandstorms are a problem in Hebei. 2. Peopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person1# is in a hurry to catch a train. Tom ...</td>\n",
       "      <td>The conversation is about the time and the tra...</td>\n",
       "      <td>#Person2# tells #Person1# that he must catch t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person1# is about to make a prank. #Person2# ...</td>\n",
       "      <td>The conversation is between two people discuss...</td>\n",
       "      <td>#Person1# tells #Person2# to pull on a strip a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Frank got a new job and is telling Judy not on...</td>\n",
       "      <td>The conversation between two people discusses ...</td>\n",
       "      <td>Frank worked for the Post Office, which offers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Tom that his novel has won the...</td>\n",
       "      <td>The conversation is about a person who receive...</td>\n",
       "      <td>#Person1# tells #Person2# that Tom's novel has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mom asks May to help to prepare for the picnic...</td>\n",
       "      <td>The conversation is about preparing for a picn...</td>\n",
       "      <td>1. Person2 wants to help Person1 prepare for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mr. Polly is tired and wants a break from work...</td>\n",
       "      <td>This is a conversation between two people, Mr....</td>\n",
       "      <td>1. Mr. Polly says he wants to buy a soft drink...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-488171ca-8e4e-4274-8e0b-aa95502001fc')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-488171ca-8e4e-4274-8e0b-aa95502001fc button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-488171ca-8e4e-4274-8e0b-aa95502001fc');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  #Person1# attends Brian's birthday party. Bria...   \n",
       "2  #Person1# thinks #Person2# has chicken pox and...   \n",
       "3  #Person2# plans to have a trip in Hebei but #P...   \n",
       "4  #Person1# is in a hurry to catch a train. Tom ...   \n",
       "5  #Person1# is about to make a prank. #Person2# ...   \n",
       "6  Frank got a new job and is telling Judy not on...   \n",
       "7  #Person1# tells Tom that his novel has won the...   \n",
       "8  Mom asks May to help to prepare for the picnic...   \n",
       "9  Mr. Polly is tired and wants a break from work...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  The conversation involves a memo about restric...   \n",
       "1  The conversation is between two people, Person...   \n",
       "2  The conversation between two people is about a...   \n",
       "3  1. Person1 and Person2 are discussing a trip t...   \n",
       "4  The conversation is about the time and the tra...   \n",
       "5  The conversation is between two people discuss...   \n",
       "6  The conversation between two people discusses ...   \n",
       "7  The conversation is about a person who receive...   \n",
       "8  The conversation is about preparing for a picn...   \n",
       "9  This is a conversation between two people, Mr....   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  1. Ms. Dawson takes a dictation for Mr. Person...  \n",
       "1  Brian is happy with the party and thanks every...  \n",
       "2  1. #Person1# thinks #Person2# has chicken pox....  \n",
       "3  1. Sandstorms are a problem in Hebei. 2. Peopl...  \n",
       "4  #Person2# tells #Person1# that he must catch t...  \n",
       "5  #Person1# tells #Person2# to pull on a strip a...  \n",
       "6  Frank worked for the Post Office, which offers...  \n",
       "7  #Person1# tells #Person2# that Tom's novel has...  \n",
       "8  1. Person2 wants to help Person1 prepare for t...  \n",
       "9  1. Mr. Polly says he wants to buy a soft drink...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", 200)  # ou une valeur assez grande\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276cdaf68df64a169acc9f81e68ce79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE - Modèle original vs humain:\n",
      "{'rouge1': np.float64(0.10907171630490317), 'rouge2': np.float64(0.036596896170629264), 'rougeL': np.float64(0.09758096481104175), 'rougeLsum': np.float64(0.09421380763834479)}\n",
      "\n",
      "ROUGE - Modèle PEFT vs humain:\n",
      "{'rouge1': np.float64(0.31826960944754035), 'rouge2': np.float64(0.15902028857012673), 'rougeL': np.float64(0.2914808094719181), 'rougeLsum': np.float64(0.28491432032846364)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-fb7583ea-8d60-45ae-8fc1-fc208c909e6b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>original</th>\n",
       "      <td>0.109072</td>\n",
       "      <td>0.036597</td>\n",
       "      <td>0.097581</td>\n",
       "      <td>0.094214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peft</th>\n",
       "      <td>0.318270</td>\n",
       "      <td>0.159020</td>\n",
       "      <td>0.291481</td>\n",
       "      <td>0.284914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb7583ea-8d60-45ae-8fc1-fc208c909e6b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-fb7583ea-8d60-45ae-8fc1-fc208c909e6b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-fb7583ea-8d60-45ae-8fc1-fc208c909e6b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "original  0.109072  0.036597  0.097581   0.094214\n",
       "peft      0.318270  0.159020  0.291481   0.284914"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "rouge_original = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_stemmer=True,\n",
    "    rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"],\n",
    ")\n",
    "\n",
    "rouge_peft = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_stemmer=True,\n",
    "    rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"],\n",
    ")\n",
    "\n",
    "print(\"ROUGE - Modèle original vs humain:\")\n",
    "print(rouge_original)\n",
    "\n",
    "print(\"\\nROUGE - Modèle PEFT vs humain:\")\n",
    "print(rouge_peft)\n",
    "\n",
    "# Optionnel : affichage propre en tableau\n",
    "pd.DataFrame({\"original\": rouge_original, \"peft\": rouge_peft}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gain (%) par métrique (PEFT vs original):\n",
      "{'rouge1': 191.79847922979184, 'rouge2': 334.51851170304445, 'rougeL': 198.7066279128813, 'rougeLsum': 202.412488647263}\n",
      "\n",
      "ROUGE moyen (original) = 0.0844\n",
      "ROUGE moyen (peft)     = 0.2634\n",
      "Gain moyen (%)         = 212.24%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c422a3a8-287f-4320-8027-1b05310baa9e\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>peft</th>\n",
       "      <th>gain_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rouge1</th>\n",
       "      <td>0.109072</td>\n",
       "      <td>0.318270</td>\n",
       "      <td>191.798479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge2</th>\n",
       "      <td>0.036597</td>\n",
       "      <td>0.159020</td>\n",
       "      <td>334.518512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeL</th>\n",
       "      <td>0.097581</td>\n",
       "      <td>0.291481</td>\n",
       "      <td>198.706628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeLsum</th>\n",
       "      <td>0.094214</td>\n",
       "      <td>0.284914</td>\n",
       "      <td>202.412489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c422a3a8-287f-4320-8027-1b05310baa9e')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-c422a3a8-287f-4320-8027-1b05310baa9e button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-c422a3a8-287f-4320-8027-1b05310baa9e');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "           original      peft    gain_pct\n",
       "rouge1     0.109072  0.318270  191.798479\n",
       "rouge2     0.036597  0.159020  334.518512\n",
       "rougeL     0.097581  0.291481  198.706628\n",
       "rougeLsum  0.094214  0.284914  202.412489"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gain de performance (%) du modèle PEFT par rapport au modèle original\n",
    "\n",
    "# Définition: gain_pct = (peft - original) / original * 100\n",
    "metrics = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "metrics = [m for m in metrics if m in rouge_original and m in rouge_peft]\n",
    "\n",
    "gain_pct = {}\n",
    "for m in metrics:\n",
    "    base = float(rouge_original[m])\n",
    "    improved = float(rouge_peft[m])\n",
    "    gain_pct[m] = np.nan if base == 0 else (improved - base) / base * 100\n",
    "\n",
    "print(\"Gain (%) par métrique (PEFT vs original):\")\n",
    "print(gain_pct)\n",
    "\n",
    "# Gain moyen sur les métriques choisies\n",
    "rouge_original_mean = float(np.mean([rouge_original[m] for m in metrics]))\n",
    "rouge_peft_mean = float(np.mean([rouge_peft[m] for m in metrics]))\n",
    "gain_mean_pct = np.nan if rouge_original_mean == 0 else (rouge_peft_mean - rouge_original_mean) / rouge_original_mean * 100\n",
    "\n",
    "print(f\"\\nROUGE moyen (original) = {rouge_original_mean:.4f}\")\n",
    "print(f\"ROUGE moyen (peft)     = {rouge_peft_mean:.4f}\")\n",
    "print(f\"Gain moyen (%)         = {gain_mean_pct:.2f}%\")\n",
    "\n",
    "# Optionnel: tableau lisible\n",
    "pd.DataFrame({\n",
    "    \"original\": {m: rouge_original[m] for m in metrics},\n",
    "    \"peft\": {m: rouge_peft[m] for m in metrics},\n",
    "    \"gain_pct\": gain_pct,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons obtenu des résultats très satisfaisants ! En prenant le modèle Qwen 3 1.7B qui est beaucoup plus performant que Flan T5, le modèle de base sans fine tuning donne déjà de très bonnes réponses, mais elles sont longues et génère ensuite une autre tâche à effectuer en rapport avec le prompt de base. Avec seulement 20 étapes de fine tuning, le modèle a assimilé le type et la forme des réponses attendues et a donné un bon résumé respectant la structure de ceux qu'on lui a donné en apprentissage. Si on compare les résultats avec ROUGE, on observe un gain de performances d'environ 200% ce qui est immense. Pour conclure ce tp nous pouvons dire que le fine tuning est une étape cruciale pour spécialiser un modèle et effectué correctement cela peut donner des résultats excellents sans avoir à réentrainer un modèle depuis zero. Nous avons donc un fain de performances conséquent avec un coût très faible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
